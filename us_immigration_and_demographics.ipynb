{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration & Demographics Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project is a part of Udacity's Nanodegree Data Engineering program in which the participating students have to build an ETL process. The purpose of the data engineering capstone project is to give a chance to combine what is learned throughout the program and implement. For this project I have chosen to complete the capstone project provided by Udacity itself.\n",
    "\n",
    "This project intends to analyze the immigration and demographics data of US. Using the datasets, we can answer queries like:\n",
    "\n",
    "*  What mode of transport is highly used for immigration\n",
    "*  Demographics dataset should be directly proportional to immigration dataset\n",
    "*  What role does global temperature play in immigration\n",
    "*  What is Immigration per month or year\n",
    "*  What is the Immigration as per country of origin\n",
    "*  legal/illegal immigration\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, regexp_replace, col, substring\n",
    "from select import select\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Using the dataset of this project, we can analyze the immigration and demographics of US per state and year and how are they affected by different parameters in the dataset.\n",
    "\n",
    "#### The dataset used:\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office, [here](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "2. World Temperature Data: This dataset came from Kaggle, [here](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "3. U.S. City Demographic Data: This data comes from OpenSoft and contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000, [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "4. Airport Code Table: This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code, [here](https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "I have completed this project using Apache Spark, PostgreSQL and Apache Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: The following steps represent data gathering from different sources, their assessment and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/23 12:48:23 WARN Utils: Your hostname, Samars-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.187 instead (on interface en0)\n",
      "22/09/23 12:48:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/09/23 12:48:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating spark session and including postgresql jar\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.5.0.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.5.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading database config values from dl.cfg\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"/Users/samar/airflow/dl.cfg\")\n",
    "host=config['POSTGRES']['HOST']\n",
    "port=config['POSTGRES']['PORT']\n",
    "dbname=config['POSTGRES']['DBNAME']\n",
    "user=config['POSTGRES']['USER']\n",
    "password=config['POSTGRES']['PASSWORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/23 12:48:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid    |i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum        |fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|4.0   |245.0 |438.0 |LOS    |20574.0|1.0    |CA     |20582.0|40.0  |1.0    |1.0  |20160430|SYD     |null |G      |O      |null   |M      |1976.0 |10292016|F     |null  |QF     |9.495387003E10|00011|B1      |\n",
      "|5748518.0|2016.0|4.0   |245.0 |438.0 |LOS    |20574.0|1.0    |NV     |20591.0|32.0  |1.0    |1.0  |20160430|SYD     |null |G      |O      |null   |M      |1984.0 |10292016|F     |null  |VA     |9.495562283E10|00007|B1      |\n",
      "|5748519.0|2016.0|4.0   |245.0 |438.0 |LOS    |20574.0|1.0    |WA     |20582.0|29.0  |1.0    |1.0  |20160430|SYD     |null |G      |O      |null   |M      |1987.0 |10292016|M     |null  |DL     |9.495640653E10|00040|B1      |\n",
      "|5748520.0|2016.0|4.0   |245.0 |438.0 |LOS    |20574.0|1.0    |WA     |20588.0|29.0  |1.0    |1.0  |20160430|SYD     |null |G      |O      |null   |M      |1987.0 |10292016|F     |null  |DL     |9.495645143E10|00040|B1      |\n",
      "|5748521.0|2016.0|4.0   |245.0 |438.0 |LOS    |20574.0|1.0    |WA     |20588.0|28.0  |1.0    |1.0  |20160430|SYD     |null |G      |O      |null   |M      |1988.0 |10292016|M     |null  |DL     |9.495638813E10|00040|B1      |\n",
      "|5748522.0|2016.0|4.0   |245.0 |464.0 |HHW    |20574.0|1.0    |HI     |20579.0|57.0  |2.0    |1.0  |20160430|ACK     |null |G      |O      |null   |M      |1959.0 |10292016|M     |null  |NZ     |9.498180283E10|00010|B2      |\n",
      "|5748523.0|2016.0|4.0   |245.0 |464.0 |HHW    |20574.0|1.0    |HI     |20586.0|66.0  |2.0    |1.0  |20160430|ACK     |null |G      |O      |null   |M      |1950.0 |10292016|F     |null  |NZ     |9.497968993E10|00010|B2      |\n",
      "|5748524.0|2016.0|4.0   |245.0 |464.0 |HHW    |20574.0|1.0    |HI     |20586.0|41.0  |2.0    |1.0  |20160430|ACK     |null |G      |O      |null   |M      |1975.0 |10292016|F     |null  |NZ     |9.497974673E10|00010|B2      |\n",
      "|5748525.0|2016.0|4.0   |245.0 |464.0 |HOU    |20574.0|1.0    |FL     |20581.0|27.0  |2.0    |1.0  |20160430|ACK     |null |G      |O      |null   |M      |1989.0 |10292016|M     |null  |NZ     |9.497324663E10|00028|B2      |\n",
      "|5748526.0|2016.0|4.0   |245.0 |464.0 |LOS    |20574.0|1.0    |CA     |20581.0|26.0  |2.0    |1.0  |20160430|ACK     |null |G      |O      |null   |M      |1990.0 |10292016|F     |null  |NZ     |9.501354793E10|00002|B2      |\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading US Immigration data\n",
    "df = spark.read.parquet(\"data/sas_data/\")\n",
    "df.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write data from spark dataframe to postgresql\n",
    "df.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_immigration\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{user}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-------------------------+-----+\n",
      "|City            |State        |Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race                     |Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-------------------------+-----+\n",
      "|Silver Spring   |Maryland     |33.8      |40601          |41862            |82463           |1562              |30908       |2.6                   |MD        |Hispanic or Latino       |25924|\n",
      "|Quincy          |Massachusetts|41.0      |44129          |49500            |93629           |4147              |32935       |2.39                  |MA        |White                    |58723|\n",
      "|Hoover          |Alabama      |38.5      |38040          |46799            |84839           |4819              |8229        |2.58                  |AL        |Asian                    |4759 |\n",
      "|Rancho Cucamonga|California   |34.5      |88127          |87105            |175232          |5821              |33878       |3.18                  |CA        |Black or African-American|24437|\n",
      "|Newark          |New Jersey   |34.6      |138040         |143873           |281913          |5829              |86253       |2.73                  |NJ        |White                    |76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+-------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading US Demographics data\n",
    "df = spark.read.csv('data/us-cities-demographics.csv', header=True, sep=';')\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+--------------+------------+------------------+----------+-------------------------+-----+\n",
      "|city            |state        |median_age|male_population|female_population|total_population|no_of_veterans|foreign_born|avg_household_size|state_code|race                     |count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+--------------+------------+------------------+----------+-------------------------+-----+\n",
      "|Silver Spring   |Maryland     |33.8      |40601          |41862            |82463           |1562          |30908       |2.6               |MD        |Hispanic or Latino       |25924|\n",
      "|Quincy          |Massachusetts|41.0      |44129          |49500            |93629           |4147          |32935       |2.39              |MA        |White                    |58723|\n",
      "|Hoover          |Alabama      |38.5      |38040          |46799            |84839           |4819          |8229        |2.58              |AL        |Asian                    |4759 |\n",
      "|Rancho Cucamonga|California   |34.5      |88127          |87105            |175232          |5821          |33878       |3.18              |CA        |Black or African-American|24437|\n",
      "|Newark          |New Jersey   |34.6      |138040         |143873           |281913          |5829          |86253       |2.73              |NJ        |White                    |76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+--------------+------------+------------------+----------+-------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming column names\n",
    "\n",
    "demographics = df.withColumnRenamed('City','city')\\\n",
    "                .withColumnRenamed('State','state')\\\n",
    "                .withColumnRenamed('Median Age','median_age')\\\n",
    "                .withColumnRenamed('Male Population','male_population')\\\n",
    "                .withColumnRenamed('Female Population','female_population')\\\n",
    "                .withColumnRenamed('Total Population','total_population')\\\n",
    "                .withColumnRenamed('Number of Veterans','no_of_veterans')\\\n",
    "                .withColumnRenamed('Foreign-born','foreign_born')\\\n",
    "                .withColumnRenamed('Average Household Size','avg_household_size')\\\n",
    "                .withColumnRenamed('Size','size')\\\n",
    "                .withColumnRenamed('State Code','state_code')\\\n",
    "                .withColumnRenamed('Race','race')\\\n",
    "                .withColumnRenamed('Count','count')\n",
    "demographics.show(5, False)\n",
    "demographics.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "demographics.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_demographics\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Airport codes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+------------------+-----------------+\n",
      "|ident|type         |name                              |elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates                          |latitude          |longitude        |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+------------------+-----------------+\n",
      "|00A  |heliport     |Total Rf Heliport                 |11          |NA       |US         |US-PA     |Bensalem    |00A     |null     |00A       |-74.93360137939453, 40.07080078125   |-74.93360137939453|40.07080078125   |\n",
      "|00AA |small_airport|Aero B Ranch Airport              |3435        |NA       |US         |US-KS     |Leoti       |00AA    |null     |00AA      |-101.473911, 38.704022               |-101.473911       |38.704022        |\n",
      "|00AK |small_airport|Lowell Field                      |450         |NA       |US         |US-AK     |Anchor Point|00AK    |null     |00AK      |-151.695999146, 59.94919968          |-151.695999146    |59.94919968      |\n",
      "|00AL |small_airport|Epps Airpark                      |820         |NA       |US         |US-AL     |Harvest     |00AL    |null     |00AL      |-86.77030181884766, 34.86479949951172|-86.77030181884766|34.86479949951172|\n",
      "|00AR |closed       |Newport Hospital & Clinic Heliport|237         |NA       |US         |US-AR     |Newport     |null    |null     |null      |-91.254898, 35.6087                  |-91.254898        |35.6087          |\n",
      "+-----+-------------+----------------------------------+------------+---------+-----------+----------+------------+--------+---------+----------+-------------------------------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading and column modification in Airport code data\n",
    "df = spark.read.csv('data/airport-codes_csv.csv', header=True, inferSchema=True)\n",
    "airport_codes=df.withColumn('latitude', split('coordinates', ', ').getItem(0))\\\n",
    "    .withColumn('longitude', split('coordinates', ', ').getItem(1))\n",
    "\n",
    "airport_codes.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "airport_codes.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_airport_codes\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Global temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt                 |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-02-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-03-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-04-01 00:00:00|5.7879999999999985|3.6239999999999997           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-05-01 00:00:00|10.644            |1.2830000000000001           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-06-01 00:00:00|14.050999999999998|1.347                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-07-01 00:00:00|16.082            |1.396                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-08-01 00:00:00|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "+-------------------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading Global temperature data\n",
    "file_name = 'data/GlobalLandTemperaturesByCity.csv'\n",
    "df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------------------+-----+-------+--------+---------+\n",
      "|date               |avg_temperature   |avg_temperature_uncertainty|city |country|latitude|longitude|\n",
      "+-------------------+------------------+---------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|6.068             |1.7369999999999999         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01 00:00:00|null              |null                       |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01 00:00:00|null              |null                       |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-02-01 00:00:00|null              |null                       |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-03-01 00:00:00|null              |null                       |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-04-01 00:00:00|5.7879999999999985|3.6239999999999997         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-05-01 00:00:00|10.644            |1.2830000000000001         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-06-01 00:00:00|14.050999999999998|1.347                      |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-07-01 00:00:00|16.082            |1.396                      |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-08-01 00:00:00|null              |null                       |Århus|Denmark|57.05N  |10.33E   |\n",
      "+-------------------+------------------+---------------------------+-----+-------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('dt','date')\\\n",
    "                .withColumnRenamed('AverageTemperature','avg_temperature')\\\n",
    "                .withColumnRenamed('AverageTemperatureUncertainty','avg_temperature_uncertainty')\\\n",
    "                .withColumnRenamed('City','city')\\\n",
    "                .withColumnRenamed('Country','country')\\\n",
    "                .withColumnRenamed('Latitude','latitude')\\\n",
    "                .withColumnRenamed('Longitude','longitude')\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "df.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_global_temperature\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps for I94_SAS_Labels_Descriptions\n",
    "\n",
    "This data is cleaned up using regex and have been broken down into 5 different tables:\n",
    "1. staging_i94country\n",
    "2. staging_i94port\n",
    "3. staging_i94address\n",
    "4. staging_i94mode\n",
    "5. staging_i95visa\n",
    "\n",
    "These tables are further used with immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|value                                                                                   |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|libname library 'Your file location' ;                                                  |\n",
      "|proc format library=library ;                                                           |\n",
      "|                                                                                        |\n",
      "|/* I94YR - 4 digit year */                                                              |\n",
      "|                                                                                        |\n",
      "|/* I94MON - Numeric month */                                                            |\n",
      "|                                                                                        |\n",
      "|/* I94CIT & I94RES - This format shows all the valid and invalid codes for processing */|\n",
      "|  value i94cntyl                                                                        |\n",
      "|   582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'                   |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read I94_SAS_Labels_Descriptions for further processing\n",
    "df = spark.read.text('data/I94_SAS_Labels_Descriptions.SAS')\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------------------+\n",
      "|code|country                                                  |\n",
      "+----+---------------------------------------------------------+\n",
      "|582 |MEXICO Air Sea, and Not Reported (I-94, no land arrivals)|\n",
      "|236 |AFGHANISTAN                                              |\n",
      "|101 |ALBANIA                                                  |\n",
      "|316 |ALGERIA                                                  |\n",
      "|102 |ANDORRA                                                  |\n",
      "|324 |ANGOLA                                                   |\n",
      "|529 |ANGUILLA                                                 |\n",
      "|518 |ANTIGUA-BARBUDA                                          |\n",
      "|687 |ARGENTINA                                                |\n",
      "|151 |ARMENIA                                                  |\n",
      "+----+---------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting values for staging_i94country table\n",
    "i94country = df.filter(\"value like '% =  %'\").withColumn('value', regexp_replace(col(\"value\"), \" =  \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 3) as code', 'substring(value, 5, length(value)) as country')\n",
    "i94country.show(10, False)\n",
    "i94country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "i94country.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_i94country\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------+\n",
      "|code|port                        |\n",
      "+----+----------------------------+\n",
      "|ALC |ALCAN, AK                   |\n",
      "|ANC |ANCHORAGE, AK               |\n",
      "|BAR |BAKER AAF - BAKER ISLAND, AK|\n",
      "|DAC |DALTONS CACHE, AK           |\n",
      "|PIZ |DEW STATION PT LAY DEW, AK  |\n",
      "|DTH |DUTCH HARBOR, AK            |\n",
      "|EGL |EAGLE, AK                   |\n",
      "|FRB |FAIRBANKS, AK               |\n",
      "|HOM |HOMER, AK                   |\n",
      "|HYD |HYDER, AK                   |\n",
      "+----+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting values for staging_i94port table\n",
    "i94port = df.filter(\"value like '%\\t=\\t%'\").withColumn('value', regexp_replace(col(\"value\"), \"\\t=\\t\", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" =\", \"=\"))\\\n",
    "        .selectExpr('substring(value, 0, 3) as code', 'substring(value, 5, length(value)) as port')\n",
    "i94port.show(10, False)\n",
    "i94port.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "i94port.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_i94port\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|code|mode        |\n",
      "+----+------------+\n",
      "|1   |Air         |\n",
      "|2   |Sea         |\n",
      "|3   |Land        |\n",
      "|9   |Not reported|\n",
      "+----+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting values for staging_i94mode table\n",
    "i94mode = df.filter(\"value like '\\t% = %'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"\\t\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 1) as code', 'substring(value, 3, length(value)) as mode')\n",
    "i94mode.show(i94mode.count(), False)\n",
    "i94mode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "i94mode.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_i94mode\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|code|state            |\n",
      "+----+-----------------+\n",
      "|AL  |ALABAMA          |\n",
      "|AK  |ALASKA           |\n",
      "|AZ  |ARIZONA          |\n",
      "|AR  |ARKANSAS         |\n",
      "|CA  |CALIFORNIA       |\n",
      "|CO  |COLORADO         |\n",
      "|CT  |CONNECTICUT      |\n",
      "|DE  |DELAWARE         |\n",
      "|DC  |DIST. OF COLUMBIA|\n",
      "|FL  |FLORIDA          |\n",
      "+----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting values for staging_i94address table\n",
    "i94address = df.filter(\"value like '\\t%\\\\'=\\\\'%'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"\\t\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 2) as code', 'substring(value, 4, length(value)) as state')\n",
    "i94address.show(10, False)\n",
    "i94address.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "i94address.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_i94address\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|code|type    |\n",
      "+----+--------+\n",
      "|1   |Business|\n",
      "|2   |Pleasure|\n",
      "|3   |Student |\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting values for staging_i94visa table\n",
    "i94visa = df.filter(\"value like '   _ =%'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 1) as code', 'substring(value, 3, length(value)) as type')\n",
    "i94visa.show(i94visa.count(), False)\n",
    "i94visa.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data from spark dataframe to postgres\n",
    "i94visa.write.mode(\"overwrite\").jdbc(f\"jdbc:postgresql://{host}:{port}/{dbname}\", \"staging_i94visa\",\n",
    "          properties={\"user\": f\"{user}\", \"password\": f\"{password}\", \"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I have used PostgreSQL locally for data modeling. The staging tables and ERD are as below:\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The data pipeline is scheduled to run once every month to analyze the change in the dataset. The scheduling is done via Apache Airflow.\n",
    "The steps are as follows:\n",
    "* Begin Execution\n",
    "* Run task - load_staging_tables, this will execute the us_immigration_and_demographics_staging.py script inorder to create and load the staging tables in the postgresql database\n",
    "* Run task - create_final_tables, this will create the final tables as per the ERD, described below\n",
    "* Begin loading the final tables as per ERD with Start_loading dummy operator\n",
    "* The first loading task is load_global_temperature_table as it contains a huge number of data and sometimes fails due to exitcode=<Negsignal.SIGKILL: -9>, hence to ensure that ths task is not failed by Airflow due to SIGTERM, the execution timeout for postgres has been increased to 3600 seconds. Also, other tables will start loading once this table is loaded successfully so that if it fails the entire dag run fails instantly.\n",
    "* The rest of the tables are then loaded - load_airport_codes_table, load_demographics_table, load_i94address_table, load_i94country_table, load_i94mode_table, load_i94port_table, load_i94visa_table are run simultaneously\n",
    "* Finally, the immigration table is loaded with load_immigration_table task\n",
    "* Once all the tables are correctly loaded, Run_data_quality_checks is executed to confirm data quality\n",
    "* Stop execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "##### Staging Tables:\n",
    "![Staging Tables](images/staging_diagram.png)\n",
    "\n",
    "##### ERD:\n",
    "![ERD](images/ERD.png)\n",
    "\n",
    "##### Airflow graph view:\n",
    "![DAG_GRAPH](images/DAG.png)\n",
    "\n",
    "##### Run the data pipelines to create the data model\n",
    "* Configure Postgres connection in Airflow UI, as:\n",
    "    Admin >> Connections >> **+** Add a new record >> Do as below:\n",
    "\n",
    "![Postgres conn in Airflow](images/Airflow_Postgres_Conn.png)\n",
    "\n",
    "* In order to run the data pipeline using Airflow, make sure all the directory and folder structure are in the right order and accessible by Airflow by setting the PYTHONPATH correctly or importing via sys and sys.path.insert()\n",
    "* Add postgresql-42.5.0.jar in config for Spark to use it to setup postgresql connection using jdbc\n",
    "* Launch Airflow using the command - airflow scheduler and airflow webserver\n",
    "* Access Airflow UI at localhost port-8080\n",
    "* Activate and trigger us_immigration_and_demographics_dag to execute it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code for data pipelines can be viewed in their respective python script files. Please refer that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks are performed to ensure the pipeline ran as expected. These include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks, refer the logs below:\n",
    "\n",
    "##### Logs:\n",
    "![logs](images/logs_Run_data_quality_checks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data Dictionary is as follows:\n",
    "\n",
    "\n",
    "1. The immigration dataset is a dynamic dataset that is bound to change with each run\n",
    "\n",
    "**immigration:-**\n",
    "* cicid: int8, CICID is a unique number for the immigrants. (No null values found)\\n\n",
    "* i94yr: int4, 4 digit year\n",
    "* i94mon: int4, Numeric month\n",
    "* i94cit: int4, country of citizenship\n",
    "* i94res: int4, country of residence - from where one has travelled. (No null values found)\n",
    "* i94port: varchar(5), Shows all the valid and invalid codes for Ports\n",
    "* arrdate: int8, is the Arrival Date in the USA. It is a SAS date numeric field that a permament format has not been applied. (Convert it to timestamp format)\n",
    "* i94mode: int4, Mode of arrival. There are missing values as well as not reported\n",
    "* i94addr: varchar(5), Destination address - is where the immigrants resides in USA. Invalid codes are marked as 'other'\n",
    "* depdate: int8, is the Departure Date from the USA. It is a SAS date numeric field that a permament format has not been applied. (Convert it to timestamp format)\n",
    "* i94bir: int4, Age of Respondent in Years \n",
    "* i94visa: int4, Visa codes \n",
    "* count: int4, Used for summary statistics \n",
    "* dtadfile: int8, Character Date Field - Date added to I-94 Files - CIC does not use \n",
    "* visapost: varchar(5), Department of State where Visa was issued \n",
    "* occup: varchar(5), Occupation that will be performed in U.S. \n",
    "* entdepa: char(1), Arrival Flag - admitted or paroled into the U.S.\n",
    "* entdepd: char(1), Departure Flag - Departed, lost I-94 or is deceased\n",
    "* entdepu: char(1), Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "* matflag: char(1), Match flag - Match of arrival and departure records\n",
    "* biryear: int4, 4 digit year of birth \n",
    "* dtaddto: int8, Character Date Field - Date to which admitted to U.S. (allowed to stay until) \n",
    "* gender: char(1), Non-immigrant sex \n",
    "* insnum: varchar, INS number \n",
    "* airline: varchar(5), Airline used to arrive in U.S. \n",
    "* admnum: int8, Admission Number \n",
    "* fltno: varchar(50), Flight number of Airline used to arrive in U.S. \n",
    "* visatype: varchar(5), Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "---\n",
    "\n",
    "2. As immigration dataset will change, it will also affect the change in demographics dataset\n",
    "\n",
    "**demographics:-**\n",
    "* City: varchar(50)                   \n",
    "* State: varchar(50)                   \n",
    "* median_age: int4       \n",
    "* male_population: int8       \n",
    "* female_population: int8\n",
    "* total_population: int8 \n",
    "* no_of_veterans: int8  \n",
    "* foreign_born: int8            \n",
    "* avg_household_size: float  \n",
    "* state_code: char(2)              \n",
    "* race: varchar(100)                  \n",
    "* count: int8\n",
    "---\n",
    "\n",
    "3. This dataset is not as dynamic and is not assumed to be changing as frequently\n",
    "\n",
    "**airport_codes:-**\n",
    "* ident: varchar(10)       \n",
    "* type: varchar(50)         \n",
    "* name: varchar(50)        \n",
    "* elevation_ft: float \n",
    "* continent: varchar(50)    \n",
    "* iso_country: varchar(50)     \n",
    "* iso_region: varchar(50)      \n",
    "* municipality: varchar(50)   \n",
    "* gps_code: varchar(50)        \n",
    "* iata_code: varchar(50)       \n",
    "* local_code: varchar(50)\n",
    "* coordinates: varchar(100) - Here, the coordinates have been further split into latitude and longitude \n",
    "* latitude: int8\n",
    "* longitude: int8  \n",
    "---\n",
    "\n",
    "4. This is a dynamic dataset expected to change frequently as per date\n",
    "\n",
    "**global_temperature:-**\n",
    "* date: timestamp\n",
    "* avg_temperature: float\n",
    "* avg_temperature_uncertainty: float\n",
    "* city: varchar(100)\n",
    "* country: varchar(100)\n",
    "* latitude: varchar(100)\n",
    "* longitude: varchar(100)\n",
    "---\n",
    "\n",
    "5. This is a constant dataset. Expected change is rare.\n",
    "\n",
    "**i94visa:** \n",
    "* code: int4, numeric code of visa type\n",
    "* type: varchar(50), type of visa\n",
    "---\n",
    "\n",
    "6. This is a constant dataset. Expected change is rare\n",
    "\n",
    "**i94address_state:**\n",
    "* code: varchar(5), contains the two-letter state code along with numeric invalid state-code identified\n",
    "* state: varchar(100), US-state\n",
    "---\n",
    "\n",
    "7. This is a constant dataset. Expected change is rare\n",
    "\n",
    "**i94mode:**\n",
    "* code: int4, numeric code for mode of transport\n",
    "* mode: varchar(100), mode of transport\n",
    "---\n",
    "\n",
    "8. This is a constant dataset. Expected change is rare\n",
    "\n",
    "**i94port:**\n",
    "* code: varchar(5)\n",
    "* port: varchar(100)\n",
    "---\n",
    "\n",
    "9.This is a constant dataset. Expected change is rare\n",
    "\n",
    "**i94country:**\n",
    "* code: int4\n",
    "* country: varchar(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rationale for the choice of tools & technologies for the project -\n",
    "I have completed the project using Apache spark, Apache Airflow and PostgreSQL.\n",
    "* Spark is capable of handling a huge amount of data at a time, distributed across a cluster of thousands of cooperating physical or virtual servers. It has an extensive set of developer libraries and APIs. Supports languages such as Java, Python, R, and Scala.\n",
    "* Airflow - Makes data pipelines as DAGs manageable. Task management, scheduling as code, and visualization of data pipelines' dependencies, progress, logs, code, trigger tasks, and success status can be easily handled.\n",
    "* PostgreSQL - It is a powerful, open source object-relational database system. Used it for storing project data and its analysis\n",
    "\n",
    "Please Note that, I could have also implemented the project using AWS redshift and S3 instead. However, chose to use Postgresql locally as I didn't have enough Udacity's AWS credits left. Nonetheless I got a good grasp of it in the DWH, Spark and Airflow projects of the Nanodegree course.\n",
    "\n",
    "2. How often the data should be updated and why - The data should be updated as per business requirement. However, Once every month should also be good to detect any kind of change in data\n",
    "\n",
    "3. Describe your approach to the following scenarios:\n",
    "* The data was increased by 100x - Increase the cluster size if using AWS (EMR, Redshift). Increase timeout period of DAG-RUN in Apache Airflow, so that it doesnt throw out-of-memory error. One can also use indexing in data storage, make use of parquet formats and partitioning of data\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day - Make changes in Airflow schedule as per requirement\n",
    "* The database needed to be accessed by 100+ people - users with specific roles and access crredentials can be created. For eg. give read-only access, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analytical SQL Queries for the dataset:\n",
    "\n",
    "1. **Top 5 highest number of migrants from a particular country**\n",
    "\n",
    ">> select count(cicid) as num, i94cit , c.country from immigration i join i94country c\n",
    "ON (i.i94cit = c.code)\n",
    "group by i94cit , c.country\n",
    "order by num desc\n",
    "limit 5\n",
    "\n",
    "![top_5_highest_number_of_immigrants](images/top_5_country.png)\n",
    "\n",
    "2. **Total immigrants per mode**\n",
    "\n",
    ">> select count(i.cicid), i.i94mode, m.mode from immigration i join i94mode m\n",
    "on( i.i94mode = m.code)\n",
    "group by i.i94mode, m.mode\n",
    "order by count(cicid) desc\n",
    "\n",
    "![immigrants_per_mode](images/immigrants_per_mode.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
