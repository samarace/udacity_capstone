{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, regexp_replace, col, substring\n",
    "from select import select\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 14:17:43 WARN Utils: Your hostname, Samars-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.188 instead (on interface en0)\n",
      "22/09/03 14:17:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 14:17:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # I94 Immigration Data pull\n",
    "# spark = SparkSession.builder.\\\n",
    "# config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "# .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data='output_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading US Immigration data\n",
    "immigration_data = spark.read.parquet(\"sas_data\")\n",
    "print(immigration_data.count())\n",
    "immigration_data.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading US Demographics data\n",
    "\n",
    "demographic_data = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "print(demographic_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading US airport codes data\n",
    "airport_code_data = pd.read_csv('airport-codes_csv.csv')\n",
    "print(airport_code_data.info())\n",
    "airport_code_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   dt                             object \n",
      " 1   AverageTemperature             float64\n",
      " 2   AverageTemperatureUncertainty  float64\n",
      " 3   City                           object \n",
      " 4   Country                        object \n",
      " 5   Latitude                       object \n",
      " 6   Longitude                      object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599207</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>11.464</td>\n",
       "      <td>0.236</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599208</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>15.043</td>\n",
       "      <td>0.261</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599209</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>18.775</td>\n",
       "      <td>0.193</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599210</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.025</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8599212 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "0        1743-11-01               6.068                          1.737   \n",
       "1        1743-12-01                 NaN                            NaN   \n",
       "2        1744-01-01                 NaN                            NaN   \n",
       "3        1744-02-01                 NaN                            NaN   \n",
       "4        1744-03-01                 NaN                            NaN   \n",
       "...             ...                 ...                            ...   \n",
       "8599207  2013-05-01              11.464                          0.236   \n",
       "8599208  2013-06-01              15.043                          0.261   \n",
       "8599209  2013-07-01              18.775                          0.193   \n",
       "8599210  2013-08-01              18.025                          0.298   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "           City      Country Latitude Longitude  \n",
       "0         Århus      Denmark   57.05N    10.33E  \n",
       "1         Århus      Denmark   57.05N    10.33E  \n",
       "2         Århus      Denmark   57.05N    10.33E  \n",
       "3         Århus      Denmark   57.05N    10.33E  \n",
       "4         Århus      Denmark   57.05N    10.33E  \n",
       "...         ...          ...      ...       ...  \n",
       "8599207  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599208  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599209  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599210  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599211  Zwolle  Netherlands   52.24N     5.26E  \n",
       "\n",
       "[8599212 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading Global temperature data\n",
    "global_temperature_data = pd.read_csv('temp_data/GlobalLandTemperaturesByCity.csv')\n",
    "\n",
    "# fname = ('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "# global_temperature_data = pd.read_csv(fname)\n",
    "\n",
    "print(global_temperature_data.info())\n",
    "global_temperature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 14:27:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 328653 ms exceeds timeout 120000 ms\n",
      "22/09/03 14:27:25 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "22/09/03 14:30:29 WARN TaskSetManager: Stage 0 contains a task of very large size (30737 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"refresh progress\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 14:30:32 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 5.0 in stage 0.0 (TID 5)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2145)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2181)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:676)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$1(ParallelCollectionRDD.scala:73)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition$$Lambda$2371/0x00000008018e3730.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:69)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800ec5400.invokeSpecial(LambdaForm$DMH)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$MH/0x000000080115c800.invoke(LambdaForm$MH)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(DirectMethodHandleAccessor.java:155)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2447)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:538)\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 9.0 in stage 0.0 (TID 9)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2145)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2181)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:676)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$1(ParallelCollectionRDD.scala:73)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition$$Lambda$2371/0x00000008018e3730.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:69)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800ec5400.invokeSpecial(LambdaForm$DMH)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$MH/0x000000080115c800.invoke(LambdaForm$MH)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(DirectMethodHandleAccessor.java:155)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2447)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:538)\n",
      "22/09/03 14:30:32 ERROR Executor: Exception in task 10.0 in stage 0.0 (TID 10)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 0.0 (TID 3),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 0.0 (TID 9),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 0.0 (TID 0),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2145)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2181)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:676)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$1(ParallelCollectionRDD.scala:73)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition$$Lambda$2371/0x00000008018e3730.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:69)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800ec5400.invokeSpecial(LambdaForm$DMH)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$MH/0x000000080115c800.invoke(LambdaForm$MH)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(DirectMethodHandleAccessor.java:155)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2447)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:538)\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 10.0 in stage 0.0 (TID 10),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 0.0 (TID 8),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newArray(Native Method)\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2145)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2181)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1745)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:676)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$1(ParallelCollectionRDD.scala:73)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition$$Lambda$2371/0x00000008018e3730.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:69)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$DMH/0x0000000800ec5400.invokeSpecial(LambdaForm$DMH)\n",
      "\tat java.base/java.lang.invoke.LambdaForm$MH/0x000000080115c800.invoke(LambdaForm$MH)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.invokeExact_MT(Invokers$Holder)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(DirectMethodHandleAccessor.java:155)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2447)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2630)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2481)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2281)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1757)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:538)\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 0.0 (TID 5),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 0.0 (TID 6),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 0.0 (TID 1),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/09/03 14:30:32 WARN TaskSetManager: Lost task 9.0 in stage 0.0 (TID 9) (192.168.29.188 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "22/09/03 14:30:32 ERROR TaskSetManager: Task 9 in stage 0.0 failed 1 times; aborting job\n",
      "22/09/03 14:30:32 ERROR FileFormatWriter: Aborting job 91a7e1bb-3422-4668-97e2-ffa9b1fdb554.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9) (192.168.29.188 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j.reflection does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nw/yq4dq6kn2vq9jtj6_s7xh45h0000gn/T/ipykernel_18161/1963536223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_temperature_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'global_temperature_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPythonException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptionString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPythonException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptionString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mgw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_instance_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.catalyst.parser.ParseException\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# Order matters. ParseException inherits AnalysisException.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \"java_class must be a string, a JavaClass, or a JavaObject\")\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n\u001b[0m\u001b[1;32m    465\u001b[0m         param, java_object)\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 answer[proto.CLASS_FQN_START:], self._gateway_client)\n\u001b[1;32m   1660\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} does not exist in the JVM\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_fqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j.reflection does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(global_temperature_data)\n",
    "df.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'global_temperature_data'), partitionBy=['dt', 'Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('I94_SAS_Labels_Descriptions.SAS')\n",
    "df.show(df.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94country = df.filter(\"value like '% =  %'\").withColumn('value', regexp_replace(col(\"value\"), \" =  \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 3) as code', 'substring(value, 5, length(value)) as country')\n",
    "i94country.show(i94country.count(), False)\n",
    "i94country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94country.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'i94country'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94port = df.filter(\"value like '%\\t=\\t%'\").withColumn('value', regexp_replace(col(\"value\"), \"\\t=\\t\", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" =\", \"=\"))\\\n",
    "        .selectExpr('substring(value, 0, 3) as code', 'substring(value, 5, length(value)) as port')\n",
    "i94port.show(i94port.count(), False)\n",
    "i94port.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94port.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'i94port'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94mode = df.filter(\"value like '\\t% = %'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"\\t\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 1) as code', 'substring(value, 3, length(value)) as mode')\n",
    "i94mode.show(i94port.count(), False)\n",
    "i94mode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94mode.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'i94mode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i94address_state = df.filter(\"value like '\\t%\\\\'=\\\\'%'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"\\t\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"'\", \"\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \" ;\", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 2) as code', 'substring(value, 4, length(value)) as state')\n",
    "i94address_state.show(i94address_state.count(), False)\n",
    "i94address_state.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94address_state.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'i94address_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94visa = df.filter(\"value like '   _ =%'\").withColumn('value', regexp_replace(col(\"value\"), \" = \", \"=\"))\\\n",
    "        .withColumn('value', regexp_replace(col(\"value\"), \"   \", \"\"))\\\n",
    "        .selectExpr('substring(value, 0, 1) as id', 'substring(value, 3, length(value)) as type')\n",
    "i94visa.show(i94visa.count(), False)\n",
    "i94visa.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94visa.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'i94visa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data Dictionary is as follows:\n",
    "\n",
    "**immigration:-**\n",
    "* cicid: int8, CICID is a unique number for the immigrants. (No null values found)\\n\n",
    "* i94yr: int4, 4 digit year\n",
    "* i94mon: int4, Numeric month\n",
    "* i94cit: int4, country of citizenship\n",
    "* i94res: int4, country of residence - from where one has travelled. (No null values found)\n",
    "* i94port: varchar(3), Shows all the valid and invalid codes for Ports\n",
    "* arrdate: int8, is the Arrival Date in the USA. It is a SAS date numeric field that a permament format has not been applied. (Convert it to timestamp format)\n",
    "* i94mode: int4, Mode of arrival. There are missing values as well as not reported\n",
    "* i94addr: varchar(10), Destination address - is where the immigrants resides in USA. Invalid codes are marked as 'other'\n",
    "* depdate: int8, is the Departure Date from the USA. It is a SAS date numeric field that a permament format has not been applied. (Convert it to timestamp format)\n",
    "* i94bir: int4, Age of Respondent in Years \n",
    "* i94visa: int4, Visa codes \n",
    "* count: int4, Used for summary statistics \n",
    "* dtadfile: int8, Character Date Field - Date added to I-94 Files - CIC does not use \n",
    "* visapost: varchar(5), Department of State where Visa was issued \n",
    "* occup: char(3), Occupation that will be performed in U.S. \n",
    "* entdepa: char(1), Arrival Flag - admitted or paroled into the U.S.\n",
    "* entdepd: char(1), Departure Flag - Departed, lost I-94 or is deceased\n",
    "* entdepu: char(1), Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "* matflag: char(1), Match flag - Match of arrival and departure records\n",
    "* biryear: int4, 4 digit year of birth \n",
    "* dtaddto: int8, Character Date Field - Date to which admitted to U.S. (allowed to stay until) \n",
    "* gender: char(1), Non-immigrant sex \n",
    "* insnum: varchar, INS number \n",
    "* airline: char(2), Airline used to arrive in U.S. \n",
    "* admnum: int8, Admission Number \n",
    "* fltno: varchar, Flight number of Airline used to arrive in U.S. \n",
    "* visatype: char(2), Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "\n",
    "**Demographics:-**\n",
    "* City: varchar(50)                   \n",
    "* State: varchar(50)                   \n",
    "* Median Age: int4       \n",
    "* Male Population: int8       \n",
    "* Female Population: int8\n",
    "* Total Population: int8 \n",
    "* Number of Veterans: int8  \n",
    "* Foreign-born: int8            \n",
    "* Average Household Size: float  \n",
    "* State Code: char(2)              \n",
    "* Race: varchar(100)                  \n",
    "* Count: int8\n",
    "\n",
    "**Airport:-**\n",
    "* identity: varchar(10)       \n",
    "* type: varchar(50)         \n",
    "* name: varchar(50)        \n",
    "* elevation_ft: float \n",
    "* continent: varchar(50)    \n",
    "* iso_country: varchar(50)     \n",
    "* iso_region: varchar(50)      \n",
    "* municipality: varchar(50)   \n",
    "* gps_code: varchar(50)        \n",
    "* iata_code: varchar(50)       \n",
    "* local_code: varchar(50)   \n",
    "* coordinates: varchar  \n",
    "\n",
    "**Temperature:-**\n",
    "* dt: timestamp\n",
    "* AverageTemperature: float\n",
    "* AverageTemperatureUncertainty: float\n",
    "* City: varchar(100)\n",
    "* Country: varchar(100)\n",
    "* Latitude: varchar(100)\n",
    "* Longitude: varchar(100)\n",
    "\n",
    "**i94visa:** [add columns, datatypes and description]\n",
    "\n",
    "**i94address_state:**\n",
    "\n",
    "**i94mode:**\n",
    "\n",
    "**i94port:**\n",
    "\n",
    "**i94country:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATEADD(day, cast(arrdate as int),'1960-01-01') as arriv_date,\n",
    "\n",
    "# I think the problem is that I was using the sparksql date_add instead of the postgresql dateadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
